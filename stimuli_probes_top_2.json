{
	"150472": {
		"cue": "pizza",
		"story": "Two women lean in with open mouths toward a large slice of pizza on a paper plate, each taking a bite from it.",
		"encoding_questions": [
			{
				"question": "How many women are in the scene?",
				"options": ["one", "two", "three", "four"],
				"answer": "two"
			},
			{
				"question": "Where is the pizza slice located?",
				"options": ["on table", "on metal tray", "in pizza box", "on paper plate"],
				"answer": "on paper plate"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"women photograph pizza slice",
					"women bite pizza slice",
					"women carry pizza boxes",
					"women cut large pizza"
				],
				"answer": "women bite pizza slice"
			}
		]
	},
	"2344622": {
		"cue": "cat",
		"story": "A person is typing on a keyboard. A cat is resting its head next to the person's arm on top of a blue blanket, near the keyboard.",
		"encoding_questions": [
			{
				"question": "Where is the cat positioned relative to other scene elements?",
				"options": [
					"next to keyboard",
					"under blanket",
					"on person's arm",
					"on keyboard"
				],
				"answer": "next to keyboard"
			},
			{
				"question": "What is the cat resting its head on?",
				"options": ["plush toy", "keyboard edge", "blue blanket", "person's arm"],
				"answer": "blue blanket"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"cat paws at blanket",
					"cat lies on person's arm",
					"cat paws at keyboard",
					"cat lies on blanket"
				],
				"answer": "cat lies on blanket"
			}
		]
	},
	"2345035": {
		"cue": "football",
		"story": "A girl in a blue jersey runs towards a football while a girl in a white jersey runs alongside and challenges her for it.",
		"encoding_questions": [
			{
				"question": "Who runs toward the football in the scene?",
				"options": ["girl and boy", "girls", "men", "boys"],
				"answer": "girls"
			},
			{
				"question": "What are the girls doing with the ball?",
				"options": ["kicking", "bouncing", "catching", "chasing"],
				"answer": "chasing"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"girls run toward football",
					"goalkeeper catches football",
					"boy slides for football",
					"man kicks football into goal"
				],
				"answer": "girls run toward football"
			}
		]
	},
	"2347025": {
		"cue": "umbrella",
		"story": "A woman carries a baby on her back. The baby holds up a pink umbrella above them.",
		"encoding_questions": [
			{
				"question": "Who is holding the umbrella in the scene?",
				"options": ["man", "woman", "baby", "girl"],
				"answer": "baby"
			},
			{
				"question": "Where is the baby in relation to the mother?",
				"options": ["next to her", "on back", "sitting on shoulders", "in arms"],
				"answer": "on back"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"baby lies under umbrella",
					"woman holds umbrella over baby",
					"baby drags pink umbrella",
					"baby holds onto umbrella"
				],
				"answer": "baby holds onto umbrella"
			}
		]
	},
	"2347400": {
		"cue": "wine",
		"story": "Two men sit on a couch, each holding a wine glass near their faces and smelling the wine.",
		"encoding_questions": [
			{
				"question": "What are the men doing with the wine?",
				"options": ["clinking", "smelling", "pouring", "sipping"],
				"answer": "smelling"
			},
			{
				"question": "What are the men sitting on in the scene?",
				"options": ["chairs", "table", "couch", "bench"],
				"answer": "couch"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"men sip from wine glasses",
					"men grab wine bottles",
					"men smell wine glasses",
					"men clink wine glasses"
				],
				"answer": "men smell wine glasses"
			}
		]
	},
	"2348899": {
		"cue": "tie",
		"story": "A woman touches/holds the seated man's striped tie while he smiles. A man in a suit leans in and gestures toward the seated man during their conversation.",
		"encoding_questions": [
			{
				"question": "What is the woman doing in the scene?",
				"options": [
					"touching striped tie",
					"folding a striped tie",
					"presenting striped tie as a gift",
					"showing striped tie to man"
				],
				"answer": "touching striped tie"
			},
			{
				"question": "From your point of view, where is the woman in the scene?",
				"options": ["at the back", "up front", "on the left", "on the right"],
				"answer": "on the right"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"woman touches man's striped tie",
					"woman gives man a striped tie",
					"man ties another man's striped tie",
					"man holds his striped tie"
				],
				"answer": "woman touches man's striped tie"
			}
		]
	},
	"2349475": {
		"cue": "blender",
		"story": "A man uses a bike to power a blender while a woman holds the lid of the blender filled with liquid.",
		"encoding_questions": [
			{
				"question": "How is the woman securing the blender?",
				"options": [
					"holding the blender base",
					"holding the blender handle",
					"holding the blender glass",
					"holding the blender lid"
				],
				"answer": "holding the blender lid"
			},
			{
				"question": "From your point of view, where is the man who's powering the blender?",
				"options": [
					"behind it",
					"to the left of it",
					"to the right of it",
					"in front of it"
				],
				"answer": "to the left of it"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"woman opens blender lid",
					"woman cranks blender handle",
					"woman presses blender button",
					"woman holds onto blender"
				],
				"answer": "woman holds onto blender"
			}
		]
	},
	"2350974": {
		"cue": "drink",
		"story": "A child drinks from a bottle while standing next to a red car. The child rests a hand on the car door.",
		"encoding_questions": [
			{
				"question": "What is the child's hand touching?",
				"options": ["red car", "grey motorcycle", "grey car", "red motorcycle"],
				"answer": "red car"
			},
			{
				"question": "What is the child in between?",
				"options": [
					"blue car and red car",
					"red car and blue motorcycle",
					"grey car and red motorcycle",
					"blue motorcycle and red motorcycle"
				],
				"answer": "red car and blue motorcycle"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"child drinks beside stroller",
					"child drinks beside car",
					"adult drinks beside train",
					"adult drinks beside bus"
				],
				"answer": "child drinks beside car"
			}
		]
	},
	"2356125": {
		"cue": "luggage",
		"story": "Two uniformed women push/move large luggage along an airport conveyor belt, while two other uniformed staff stand nearby.",
		"encoding_questions": [
			{
				"question": "Where is the luggage being moved to?",
				"options": [
					"onto xray",
					"onto baggage cart",
					"onto conveyor belt",
					"off of xray"
				],
				"answer": "onto conveyor belt"
			},
			{
				"question": "How many uniformed staff are moving the luggage?",
				"options": ["one", "two", "three", "four"],
				"answer": "two"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"staff tag luggage at a counter",
					"staff load luggage onto a cart",
					"staff load luggage onto a conveyor belt",
					"staff load luggage into an xray"
				],
				"answer": "staff load luggage onto a conveyor belt"
			}
		]
	},
	"2360001": {
		"cue": "clock",
		"story": "Two workers are lifting a large round clock onto a building facade, with one of them steadying it while the other attaches it to the bracket and weaves through a cable.",
		"encoding_questions": [
			{
				"question": "Where are the men?",
				"options": ["on balcony", "on roof edge", "on scaffold", "on ground"],
				"answer": "on scaffold"
			},
			{
				"question": "What are the workers doing to the clock?",
				"options": ["measuring it", "painting it", "wiping it", "raising it"],
				"answer": "raising it"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"workers paint old clock",
					"workers raise ladder up to clock",
					"workers lift large clock",
					"workers wipe large clock face"
				],
				"answer": "workers lift large clock"
			}
		]
	},
	"2362277": {
		"cue": "frisbee",
		"story": "A player in white runs while attempting to throw a frisbee. A player in red closely blocks him from throwing the frisbee.",
		"encoding_questions": [
			{
				"question": "How are the frisbee defender's arms positioned?",
				"options": [
					"both below waist",
					"one above ahead; the other below waist",
					"both out to the side",
					"both high above head"
				],
				"answer": "both out to the side"
			},
			{
				"question": "From your point of view, on which side is the frisbee thrower in the scene?",
				"options": [
					"behind defender",
					"on the left",
					"in front of defender",
					"on the right"
				],
				"answer": "on the left"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"player catches frisbee at waist",
					"player runs towards frisbee",
					"player catches frisbee overhead",
					"player throws a frisbee"
				],
				"answer": "player throws a frisbee"
			}
		]
	},
	"2362704": {
		"cue": "bears",
		"story": "Two bears are eating pieces of a pumpkin on the ground. One bear has its face in the hollow pumpkin while the other eats a chunk nearby.",
		"encoding_questions": [
			{
				"question": "How many bears are in the scene?",
				"options": ["one", "two", "three", "four"],
				"answer": "two"
			},
			{
				"question": "What are the bears doing with the pumpkin?",
				"options": ["eating", "fighting over", "pawing", "sniffing"],
				"answer": "eating"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"bears fight over pumpkin",
					"bears paw at pumpkin",
					"bears sniff pumpkin",
					"bears eat pumpkin"
				],
				"answer": "bears eat pumpkin"
			}
		]
	},
	"2363971": {
		"cue": "petting",
		"story": "A woman pets a large dog in the foreground while another woman in a chair holds a child upside down on her lap. A man also pets the dog in the foreground. The scene takes place indoors.",
		"encoding_questions": [
			{
				"question": "Where does the petting occur in the scene?",
				"options": [
					"outdoors (park)",
					"outdoors (street)",
					"indoors (living room)",
					"indoors (petting zoo)"
				],
				"answer": "indoors (living room)"
			},
			{
				"question": "What is the seated woman (in background) doing with the child?",
				"options": [
					"reading a book to it",
					"hugging it",
					"feeding it from plate",
					"holding it upside down"
				],
				"answer": "holding it upside down"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"child pets dog indoors",
					"woman pets dog indoors",
					"child pets horse outside",
					"woman pets cat outside"
				],
				"answer": "woman pets dog indoors"
			}
		]
	},
	"2366915": {
		"cue": "surfer",
		"story": "Two surfers ride along the same breaking wave as water sprays behind and around them.",
		"encoding_questions": [
			{
				"question": "How many surfers are riding the wave?",
				"options": ["one", "two", "three", "four"],
				"answer": "two"
			},
			{
				"question": "How large is the wave relative to the surfers?",
				"options": [
					"as tall as them",
					"twice as tall",
					"half as tall",
					"three times as tall"
				],
				"answer": "as tall as them"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"two surfers paddle toward wave",
					"two surfers walk toward wave",
					"two surfers ride a wave",
					"one surfer rides a wave"
				],
				"answer": "two surfers ride a wave"
			}
		]
	},
	"2367132": {
		"cue": "video game",
		"story": "Two men are playing a video game while holding motion controllers and moving their arms.",
		"encoding_questions": [
			{
				"question": "How many men are playing in the scene?",
				"options": ["one", "two", "three", "four"],
				"answer": "two"
			},
			{
				"question": "From THEIR point of view, in which hand(s) do the men hold the motion controllers?",
				"options": [
					"in left hand",
					"in both hands",
					"neither hand (on wrists)",
					"in right hand"
				],
				"answer": "in both hands"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"men play video game on their laptops",
					"men sit on couch and play xbox video game",
					"men play video game while standing",
					"men watch video game on TV"
				],
				"answer": "men play video game with motion controllers"
			}
		]
	},
	"2369373": {
		"cue": "chef",
		"story": "A chef places a pizza into a wood-fired oven, which has the appearance of a Greek god's open mouth.",
		"encoding_questions": [
			{
				"question": "What is the appearance of the oven?",
				"options": ["brick dome", "stone arch", "metal box", "open mouth"],
				"answer": "open mouth"
			},
			{
				"question": "What is the chef placing into the oven?",
				"options": ["pizza", "steak", "bread", "vegetable tray"],
				"answer": "pizza"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"chef pulls tray from oven",
					"chef loads bread into oven",
					"chef loads pizza into oven",
					"chef stokes oven flames"
				],
				"answer": "chef loads pizza into oven"
			}
		]
	},
	"2370664": {
		"cue": "donuts",
		"story": "Two men and a woman are taking bites of donuts. They are close together outdoors, holding the donuts up to their mouths.",
		"encoding_questions": [
			{
				"question": "How many donuts are in the scene?",
				"options": ["two", "three", "four", "five"],
				"answer": "three"
			},
			{
				"question": "Where are the people located in the scene?",
				"options": ["street", "kitchen", "restaurant", "living room"],
				"answer": "street"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"people bite into donuts",
					"man offers donut to woman",
					"people hold donuts over eyes",
					"people dip donuts in coffee"
				],
				"answer": "people bite into donuts"
			}
		]
	},
	"2371471": {
		"cue": "motorcycle",
		"story": "Two motorcyclists are airborne on dirt bikes inside a building, performing jumps above a ramp.",
		"encoding_questions": [
			{
				"question": "Which of the motorcyclists is performing a handstand?",
				"options": ["both", "neither", "one up front", "one at the back"],
				"answer": "one up front"
			},
			{
				"question": "What is nearest to the motorcyclists?",
				"options": ["concrete ramp", "dirt ramp", "grassy ground", "asphalt road"],
				"answer": "dirt ramp"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"motorcyclists jump over ramp",
					"motorcyclists pop a wheelie",
					"motorcyclists speed across road",
					"motorcyclists drive on road"
				],
				"answer": "motorcyclists jump over ramp"
			}
		]
	},
	"2374670": {
		"cue": "truck",
		"story": "A red truck sprays water onto a dirt road through nozzles at the back. A man is climbing on the back of the truck while it operates.",
		"encoding_questions": [
			{
				"question": "What vehicle sprays water in the scene?",
				"options": ["grey tractor", "red truck", "grey van", "red tractor"],
				"answer": "red truck"
			},
			{
				"question": "What is the truck spraying?",
				"options": ["fertilizer", "asphalt", "sand", "water"],
				"answer": "water"
			},
			{
				"question": "Where is the (actively moving) man positioned relative to the truck?",
				"options": ["back", "side", "front", "roof"],
				"answer": "back"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"truck pulls broken car",
					"truck fills water tank",
					"truck sprays water",
					"truck dumps gravel onto road"
				],
				"answer": "truck sprays water"
			}
		]
	},
	"2379478": {
		"cue": "milk",
		"story": "Two babies are lying on a bed, each drinking from a milk bottle.",
		"encoding_questions": [
			{
				"question": "What is the position of the babies?",
				"options": ["lying on bed", "lying on couch", "sitting on bed", "lying in crib"],
				"answer": "lying on bed"
			},
			{
				"question": "In what position are the babies drinking milk?",
				"options": ["sitting up", "lying on back", "lying on side", "lying on stomach"],
				"answer": "lying on back"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"one baby reaches for milk bottle",
					"two babies drink from milk bottles",
					"two babies reach for one milk bottle",
					"one baby drinks from a milk bottle"
				],
				"answer": "two babies drink from milk bottles"
			}
		]
	},
	"2383555": {
		"cue": "boat",
		"story": "A woman sits in a bamboo boat holding a child on her lap. A woman behind her rows the boat through the water.",
		"encoding_questions": [
			{
				"question": "Who is holding the child in the boat?",
				"options": ["boy", "woman", "man", "girl"],
				"answer": "woman"
			},
			{
				"question": "What is the woman at the back doing in the boat?",
				"options": ["waving", "rowing", "sitting", "standing"],
				"answer": "rowing"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"child sits alone in boat",
					"man steps onto a boat",
					"two women row a boat",
					"woman holds child on boat"
				],
				"answer": "woman holds child on boat"
			}
		]
	},
	"2385550": {
		"cue": "tangerines",
		"story": "A boy and a girl each have a tangerine in their mouth and are eating it while facing the camera.",
		"encoding_questions": [
			{
				"question": "Where are the tangerines in the scene?",
				"options": ["in front of eyes", "in hands", "on head", "in mouths"],
				"answer": "in mouths"
			},
			{
				"question": "From your point of view, where is the boy in the scene?",
				"options": ["on the left", "in the front", "on the right", "in the back"],
				"answer": "on the left"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"children pick tangerines from tree",
					"children have tangerines in mouths",
					"children hold tangerines in hands",
					"children peel tangerines"
				],
				"answer": "children have tangerines in mouths"
			}
		]
	},
	"2386442": {
		"cue": "teeth",
		"story": "A woman, a child, and a man are each brushing their teeth with toothbrushes. They are facing the camera while brushing. The man is wearing a purple shirt, the woman is wearing a striped shirt, while the child is wearing a pink checkered shirt.",
		"encoding_questions": [
			{
				"question": "Who is brushing their teeth in the scene?",
				"options": [
					"child and two women",
					"man, woman, and child",
					"man and woman",
					"man and two children"
				],
				"answer": "man, woman, and child"
			},
			{
				"question": "From your point of view, where is the child in the scene?",
				"options": ["on the right", "in the middle", "at the back", "on the left"],
				"answer": "in the middle"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"woman brushes a child's teeth",
					"child flosses teeth carefully",
					"family brushes teeth together",
					"family checks teeth in mirror"
				],
				"answer": "family brushes teeth together"
			}
		]
	},
	"2387122": {
		"cue": "polar bear",
		"story": "A polar bear swims in the water and presses against the enclosure glass while a woman on the other side takes a photo of it.",
		"encoding_questions": [
			{
				"question": "Where is the polar bear located in the scene?",
				"options": ["on sand", "on rocks", "on ice", "in water"],
				"answer": "in water"
			},
			{
				"question": "What is the polar bear doing next to the enclosure edge?",
				"options": [
					"diving underwater",
					"swimming away from glass enclosure",
					"pressing on glass enclosure",
					"sitting on snowy ground"
				],
				"answer": "pressing on glass enclosure"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"polar bear presses on glass",
					"polar bear dives underwater",
					"polar bear sits on snowy ground",
					"polar bear swims away from glass"
				],
				"answer": "polar bear presses on glass"
			}
		]
	},
	"2397102": {
		"cue": "cake",
		"story": "A woman and a man are cutting a decorated wedding cake together while the man holds a bottle in his other hand.",
		"encoding_questions": [
			{
				"question": "What are the couple doing with the cake?",
				"options": [
					"serving it",
					"cutting it",
					"posing next to it",
					"feeding each other"
				],
				"answer": "cutting it"
			},
			{
				"question": "What is the man holding besides the cake knife?",
				"options": ["beer bottle", "present", "wine glass", "paper plate"],
				"answer": "beer bottle"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"couple poses with wedding cake",
					"couple cuts cake together",
					"couple kisses beside cake",
					"couple feeds each other cake"
				],
				"answer": "couple cuts cake together"
			}
		]
	},
	"2401636": {
		"cue": "baby",
		"story": "A cat leans toward and sniffs a baby sitting beside it. A woman holds the cat and baby close while watching the baby.",
		"encoding_questions": [
			{
				"question": "Who is looking at the cat in the scene?",
				"options": ["baby", "girl", "woman", "man"],
				"answer": "baby"
			},
			{
				"question": "What is the mom holding in her arms?",
				"options": ["dog and cat", "baby and dog", "baby and cat", "baby"],
				"answer": "baby and cat"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"cat licks woman's hand",
					"woman raises up cat",
					"cat sleeps beside baby",
					"baby look at cat"
				],
				"answer": "baby looks at cat"
			}
		]
	},
	"2411268": {
		"cue": "apple",
		"story": "A child reaches up to pick a green apple from a tree while holding a red bag for collecting fruit.",
		"encoding_questions": [
			{
				"question": "What is the child holding while picking fruit?",
				"options": ["basket", "bucket", "bag", "box"],
				"answer": "bag"
			},
			{
				"question": "Where is the fruit branch relative to the child?",
				"options": ["near ground", "above head", "below head", "at head level"],
				"answer": "above head"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"child puts apple into bag",
					"child picks apple from tree",
					"child bites into apple",
					"child picks apple up from the ground"
				],
				"answer": "child picks apple from tree"
			}
		]
	},
	"2412687": {
		"cue": "dog",
		"story": "A person holds out a red frisbee toward a dog. The dog bites onto the frisbee with its mouth.",
		"encoding_questions": [
			{
				"question": "What is the dog biting onto in the scene?",
				"options": ["red bone", "red frisbee", "red boomerang", "red ball"],
				"answer": "red frisbee"
			},
			{
				"question": "What is the main action of the human in the scene?",
				"options": [
					"picking up frisbee",
					"catching frisbee",
					"holding frisbee",
					"throwing frisbee"
				],
				"answer": "holding frisbee"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"dog bites onto frisbee",
					"dog ignores offered frisbee",
					"man throws frisbee towards dog",
					"dog catches frisbee midair"
				],
				"answer": "dog bites onto frisbee"
			}
		]
	},
	"2356991": {
		"cue": "baseball",
		"story": "A player slides headfirst toward a base while another player in a glove catches a thrown ball beside the base.",
		"encoding_questions": [
			{
				"question": "How is the player on the left reaching the base?",
				"options": [
					"running towards it",
					"sliding on knees",
					"sliding feetfirst",
					"sliding headfirst"
				],
				"answer": "sliding headfirst"
			},
			{
				"question": "What is the current action of the gloved player on the base?",
				"options": [
					"catching the ball",
					"tagging the runner",
					"throwing the ball",
					"dropping the ball"
				],
				"answer": "catching the ball"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"player slides feetfirst toward base",
					"gloved player holds a ball",
					"player slides headfirst to base",
					"player runs towards base"
				],
				"answer": "player slides headfirst to base"
			}
		]
	},
	"2385851": {
		"cue": "suitcase",
		"story": "Two men walk along a covered walkway while each carries a large red suitcase.",
		"encoding_questions": [
			{
				"question": "What are the men doing with the suitcases?",
				"options": ["pulling them", "stacking them", "opening them", "carrying them"],
				"answer": "carrying them"
			},
			{
				"question": "Where are the men?",
				"options": [
					"walking in an airport terminal",
					"walking under a covered walkway",
					"walking into a hotel lobby",
					"walking along a train platform"
				],
				"answer": "walking under a covered walkway"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"men stack suitcases on cart",
					"men carry suitcases on shoulders",
					"men pull suitcases behind them",
					"man opens suitcases outside"
				],
				"answer": "men carry suitcases on shoulders"
			}
		]
	},
	"2394020": {
		"cue": "horse ride",
		"story": "Two horseriders are riding horses along the sandy shore beside the water. Both horses are patchy while the riders are wearing cowboy hats. The weather is sunny and the water is calm.",
		"encoding_questions": [
			{
				"question": "Where is the scene taking place?",
				"options": ["along beach", "along open field", "along ocean", "along stream"],
				"answer": "along beach"
			},
			{
				"question": "What are the riders wearing on their heads?",
				"options": ["sun hats", "helmets", "baseball caps", "cowboy hats"],
				"answer": "cowboy hats"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"people ride horses on road",
					"people ride horses along beach",
					"people ride horses through field",
					"people ride horses through forest"
				],
				"answer": "people ride horses along beach"
			}
		]
	},
	"2406346": {
		"cue": "circus",
		"story": "One elephant holds the tail of another elephant, ahead of it, with its trunk. The elephant up front has a star on its back. A handler walks beside the elephants holding a stick and leading the elephants forward.",
		"encoding_questions": [
			{
				"question": "Which elephant is the man/handler closest to?",
				"options": [
					"both elephants",
					"back elephant",
					"front elephant",
					"neither elephant"
				],
				"answer": "back elephant"
			},
			{
				"question": "What is one elephant doing to the other?",
				"options": [
					"spraying it with water",
					"touching its trunks",
					"nothing; its trunk is extended above its head without interaction",
					"holding its tail"
				],
				"answer": "holding its tail"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"circus elephants stand on pedestals",
					"circus elephants washed by handler",
					"circus elephants perform tricks",
					"circus elephants walk on street"
				],
				"answer": "circus elephants walk on street"
			}
		]
	},
	"2408717": {
		"cue": "surfboard",
		"story": "Four surfers walk along the shoreline while carrying their surfboards. Waves roll in front of them as they head down towards the beach.",
		"encoding_questions": [
			{
				"question": "How many surfers are in the scene?",
				"options": ["two", "three", "four", "five"],
				"answer": "four"
			},
			{
				"question": "What are the colors of the surfboards in the scene?",
				"options": [
					"blue and yellow",
					"white and yellow",
					"red and green",
					"white and red"
				],
				"answer": "white and yellow"
			},
			{
				"question": "What is the current state of the surfers? They're...",
				"options": [
					"walking towards water",
					"standing on surfboards",
					"surfing on a wave",
					"paddling in water"
				],
				"answer": "walking towards water"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"surfers stand on surfboards on the beach",
					"surfers walk towards water with surfboards",
					"surfers place surfboards into water",
					"surfers lie on surfboards"
				],
				"answer": "surfers walk towards water with surfboards"
			}
		]
	},
	"2409918": {
		"cue": "elephant",
		"story": "Two men are riding elephants while they are swimming in moderately deep water. One elephant is eating plants with its trunk.",
		"encoding_questions": [
			{
				"question": "What is the elephant on the right doing with its trunk?",
				"options": [
					"grabbing log",
					"reaching forward",
					"spraying water",
					"eating plants"
				],
				"answer": "eating plants"
			},
			{
				"question": "Where are the elephants in the scene?",
				"options": [
					"walking on land",
					"swimming in water",
					"splashing in puddle",
					"standing in mud"
				],
				"answer": "swimming in water"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"elephants walk across savannah",
					"elephants splash riders",
					"elephants drink water",
					"elephants swim in water"
				],
				"answer": "elephants swim in water"
			}
		]
	},
	"2410309": {
		"cue": "laptop",
		"story": "A woman sits on a couch using a laptop on her lap. A baby beside her touches and uses another open laptop.",
		"encoding_questions": [
			{
				"question": "What device is the woman using on the couch?",
				"options": ["tablet", "book", "phone", "laptop"],
				"answer": "laptop"
			},
			{
				"question": "How many open laptops are in the scene?",
				"options": ["one", "two", "three", "four"],
				"answer": "two"
			}
		],
		"decoding_questions": [
			{
				"question": "Which description best matches the image?",
				"options": [
					"woman and baby use laptops",
					"man types on laptop keyboard",
					"woman closes laptop lid",
					"baby touches laptop mouse"
				],
				"answer": "woman and baby use laptops"
			}
		]
	}
}
